{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb48652-e543-4392-a91f-0d28de7dcc05",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd227382-ef11-4508-a46f-1f9689f9a1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pprint import pprint\n",
    "#import nctoolkit\n",
    "# colors for lines (color blind friendly colors: https://gist.github.com/thriveth/8560036)\n",
    "CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']\n",
    "\n",
    "import matplotlib as mpl  # # Plotting\n",
    "import matplotlib.pyplot as plt  # python plotting package\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from colorspacious import cspace_converter\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "cmaps = OrderedDict()\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LATITUDE_FORMATTER, LONGITUDE_FORMATTER\n",
    "from cartopy.mpl.ticker import LatitudeFormatter, LongitudeFormatter\n",
    "from colorspacious import cspace_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1e0f6f-4aab-4f3b-8439-a1cf3c70a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cftime\n",
    "import pandas as pd\n",
    "from numpy import *\n",
    "from scipy import stats\n",
    "from netCDF4 import Dataset as nc\n",
    "from netCDF4 import num2date\n",
    "import netCDF4\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt #python plotting package\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as mcolors\n",
    "from colorspacious import cspace_converter\n",
    "from collections import OrderedDict\n",
    "cmaps = OrderedDict()\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "#import nctoolkit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cpf\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "from shapely import geometry\n",
    "from collections import namedtuple\n",
    "from shapely.geometry.polygon import LinearRing\n",
    "from shapely.geometry.polygon import Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5f9e8-d274-4588-b15e-d92b82e9974d",
   "metadata": {},
   "source": [
    "# Cali Pfleger\n",
    "## 05/25/2024\n",
    "- Model iCESM and CESM Last Millenium Ensemble \n",
    "- Climate System: Pacific Walker Circulation \n",
    "- Index: Sea Level Pressure Index (hPa)\n",
    "- Variable: d18Op (0/00) (per mille)\n",
    "- Time period: 1850-2005 \n",
    "- Forcings: Full, GHG, LULC, OZA\n",
    "\n",
    "# Calculations:\n",
    "- Index Time Series (trend line, p-value, standered error (+1/-1) ribbon and ensmeble average)\n",
    "- Climatological Average Maps\n",
    "- Global Average Time Series (standered error ribbon and ensmeble average)\n",
    "- Linear Trend Maps (95% CI significance stippling)- \n",
    "- Correlation with Index Maps (95% CI significance stippling)- \n",
    "- Effect of Forcing Maps (Correlation - PI Control Run Corrleation)\n",
    "GCM Changes in Vertical Velocity (ω) Across the Tropical Pacific.\n",
    "iCAM5 = 2xCO2 Experiment minus Modern Control, Meridional Mean (−20 to 20 latitude).Change in vertical velocity (ω) in the experiment minus the control in color filled contour plot. Black contour lines show control simulation. Sign convention is negative = upward motion, positive = downward motion. Thus, red colors in Western Pacific indicate a reduction in the magnitude of the negative values in the control compared to the experiment. Blue colors indicate less positive values in the experiment compared to the control, or less downward motion.\n",
    "\n",
    "- Vertical Profile Maps (dD(per mille), omega, temperature (C), pressure (hPa))\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e577652-fbcf-4e9b-a427-cbfba6709d37",
   "metadata": {},
   "source": [
    "# Set Paths, Parameters and Naming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbbb20a-b42f-4b34-8bc5-ae3339b35a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Paths\n",
    "basedir_atm = '/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/'\n",
    "file_prefix= '.cam.h0.' \n",
    "i_model = '/b.ie12.B1850C5CN.f19_g16.LME.' ## read in 2&3 iLME Ensembles \n",
    "i_model1 = '/b.ie12.B1850CN.f19_g16.' ## read in iLME ens 1\n",
    "model = '/b.e11.BLMTRC5CN.f19_g16.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c2288e0-a58e-49d1-a183-19b386ccec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters and Naming \n",
    "model_name ='Model iCESM and CESM '\n",
    "region = 'PWC Index ' # 'Eastern Pacific ' 'Western Pacific ' \n",
    "variable = 'SLP' #Sea Level Pressure\n",
    "period = \"Post Industrial\"# \"Full LME\" #\"Pre Industrial\",\n",
    "timep = \"_post\"# \"_pre\" # \n",
    "var_name = ['Full', 'GHG', 'LULC','OZA']#, 'ANTHRO']\n",
    "timep_title = \"Post Industrial\"#'Pre Industrial' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b60fc6-11b9-4fb8-b089-a9818674302d",
   "metadata": {},
   "source": [
    "# Index Box Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8958b667-4531-4304-b997-bcb54d561452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Dr. Samantha Stevenson UCSB\n",
    "#mask_lon = (mytos.lon >= regbox[2]) & (mytos.lon <= regbox[3])\n",
    "#mask_lat = (mytos.lat >= regbox[0]) & (mytos.lat <= regbox[1])\n",
    "#hsst=mytos.where(mask_lon & mask_lat, drop=True).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d14fd32-f3ae-46d3-a6f5-9ff93d30393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PWC Index\n",
    "def pwci(p):\n",
    "    #p = c.groupby('time.month').mean('time')\n",
    "    east=  p.sel(lat=slice(-5,5), lon=slice(210,270))  # \n",
    "    west = p.sel(lat=slice(-5,5), lon=slice(130,150))\n",
    "    avg_east = east.mean(('lat', 'lon'))\n",
    "    avg_west = west.mean(('lat', 'lon'))\n",
    "    pwc = (avg_east - avg_west).compute()\n",
    "    pwc= pwc.compute()\n",
    "    return pwc, avg_east, avg_west"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55acdd08-5b78-41fa-adfd-aefb773cfd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make geom for E and W PWC index boxes\n",
    "from shapely import geometry\n",
    "from shapely.geometry.polygon import LinearRing, Polygon\n",
    "Elats = [-5, -5, 5, 5]\n",
    "Elons = [90, 150, 150,90]\n",
    "Ering = LinearRing(list(zip(Elons, Elats)))\n",
    "Wlats = [-5, -5, 5, 5]\n",
    "Wlons = [-150,-90,-90,-150]\n",
    "Wring = LinearRing(list(zip(Wlons, Wlats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a05f171-80fd-4b64-930d-7099b40a0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for meridional mean and lat lon range \n",
    "def mer(m):\n",
    "    #m_we = weights(m)\n",
    "    mer_lat= m.sel(lat=slice(-20, 20)).mean(dim='lat')\n",
    "    mer_lon = mer_lat.sel(lon=slice(110, 270))\n",
    "    mer_clim = clim(mer_lon)\n",
    "    mer_time = mer_clim.mean(dim='time')\n",
    "    mer_avg = mer_time#.mean(dim = 'ensemble')\n",
    "    return mer_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339cbb8-8adb-4c1e-95a8-c2b907630c1d",
   "metadata": {},
   "source": [
    "# CESM converting month 01 to January "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1284a371-3256-4676-a89f-1012b2d0cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from cftime import DatetimeNoLeap\n",
    "\n",
    "# Set file year\n",
    "year = np.linspace(0,1872, 1872) \n",
    "dates = [DatetimeNoLeap(year, month, 1) for year, month in product(range(1850, 2006), range(1, 13))]\n",
    "da = xr.DataArray(np.arange(1872), coords=[dates], dims=['time'], name='time')\n",
    "## Set control year \n",
    "i_cntlyear = np.linspace(0,2400, 2400) \n",
    "i_cntldates = [DatetimeNoLeap(i_cntlyear, month, 1) for i_cntlyear, month in product(range(650, 850), range(1, 13))]\n",
    "i_cntlda =xr.DataArray(np.arange(2400), coords=[i_cntldates], dims=['time'], name='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107bf4d-ff46-4908-8364-101155642081",
   "metadata": {},
   "source": [
    "# Calculations and Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "018a0827-8630-4f46-b100-02bae234cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grid cell weighting to acount for grid cells in extratropics \n",
    "def weights(w):\n",
    "    coslat = np.cos(np.deg2rad(w.lat)) #  Take the cosine of latitude (first converting to radians)\n",
    "    weight_factor = coslat / coslat.mean(dim='lat')#  And divide by its mean value\n",
    "    computed_weight = w * weight_factor # .mean(dim=('time', 'lon', 'lat'))\n",
    "    return computed_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "784be969-55ce-4660-92e9-83d4113178ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Ensemble average \n",
    "def ensav(e):\n",
    "    ens_avg = e.mean(dim = 'ensemble')\n",
    "    return ens_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7198985-f02f-4fe3-b794-1d561ea01780",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Anomaly relative to Climatology for variable \n",
    "def clim_cntl(tc): \n",
    "    #units = tc/100 #convert d18O \n",
    "    #convert precip 1000*24*3600\n",
    "    c = tc.assign_coords({'time':(i_cntlda.time)}) #convert time dimestion to have 01 be january \n",
    "    clim_var = c.groupby('time.month').mean('time') #calculate monthly climatology\n",
    "    clim_anom = c.groupby('time.month') - clim_var # calculate the anomaly relative to the climatology \n",
    "    #anom_weights = weights(clim_anom) #weight functino to account for lat grid size \n",
    "    return clim_anom #return the anomaly calcuated by the anomaly with time and lat conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71e8754a-9d42-4cbb-99fa-1a87baac1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Ensemble average \n",
    "def clim_avg(ca):\n",
    "    #convert precip 1000*24*3600\n",
    "    c = ca.assign_coords({'time':(da.time)})\n",
    "    time_avg = c.mean('time')\n",
    "    ens_avg = time_avg.mean(dim = 'ensemble')\n",
    "    return ens_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acb33cb9-525e-4f7e-a87b-ea5f837be920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Correlation Coefficent using xr.corr \n",
    "def cor(g,t):\n",
    "    ds= []\n",
    "    fcor =xr.corr(g,t, dim = 'time') # g is the global data set and t is the time series index \n",
    "    ds.append(fcor) # add correlation coeefiecent to dataset \n",
    "    cords= xr.concat(ds, dim='ensemble')  # create ensmenble dimesion for correlation coeeficent \n",
    "    cor_avg = cords.mean('ensemble') # calculate ensemble average for correlation coeeficent \n",
    "    return cor_avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d354ab3-a03d-4aee-9154-dd76dd50d908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Files: /b.e11.BLMTRC5CN.f19_g16.Post Industrial PWC Index \n"
     ]
    }
   ],
   "source": [
    "print('Index Files: ' +  model + period ,''+ region )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de077c-c0e6-419f-a995-63e0c48ae01f",
   "metadata": {},
   "source": [
    "# Read in Index Files: PSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b7ea605-23b3-4d1a-81a1-d62ba557c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'PSL' #set output variable name for reading in file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54cbc13d-a9a7-4848-b503-12ca3610e185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.89it/s]\n",
      "100%|██████████| 9/9 [00:06<00:00,  1.50it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.24 s, sys: 1.36 s, total: 7.6 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ens = '' #Full forcing ensemble \n",
    "file_suffix= '.185001-200512.nc'\n",
    "ds = []# initialise array:\n",
    "for member in tqdm(range(1,2)):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).PSL.squeeze()\n",
    "    ds.append(member)\n",
    "    \n",
    "for member in tqdm(range(2,3+1)): #loop to read in the 1-9 ensemble members \n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix # set file path \n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens +'00' +file).PSL.squeeze() #loop\n",
    "    ds.append(member)\n",
    "\n",
    "for member in tqdm(range(1,9+1)):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+model+ ens + '00'+file).PSL.squeeze()\n",
    "    ds.append(member)\n",
    "    \n",
    "for member in tqdm(range(10,13+1)):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ model+ens +'0'+file).PSL.squeeze()\n",
    "    ds.append(member)\n",
    "    \n",
    "post_index = (xr.concat(ds, dim='ensemble')/100).assign_coords({'time':(da.time)}) #create ensemble dimension after reading in all the ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fe5b3d5-f38d-45bf-a34d-f465b38807e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 13.54it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.57it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'time': length 1965 on <this-array> and length 1872 on {'lat': 'lat', 'lon': 'lon', 'time': 'time', 'ensemble': <this-array>}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:19\u001b[0m\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/core/common.py:621\u001b[0m, in \u001b[0;36mDataWithCoords.assign_coords\u001b[0;34m(self, coords, **coords_kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_assign_results(coords_combined)\n\u001b[0;32m--> 621\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/core/coordinates.py:566\u001b[0m, in \u001b[0;36mCoordinates.update\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# special case for PandasMultiIndex: updating only its dimension coordinate\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# is still allowed but depreciated.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# It is the only case where we need to actually drop coordinates here (multi-index levels)\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# TODO: remove when removing PandasMultiIndex's dimension coordinate.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_coords(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_names \u001b[38;5;241m-\u001b[39m coords_to_align\u001b[38;5;241m.\u001b[39m_names)\n\u001b[0;32m--> 566\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/core/coordinates.py:842\u001b[0m, in \u001b[0;36mDataArrayCoordinates._update_coords\u001b[0;34m(self, coords, indexes)\u001b[0m\n\u001b[1;32m    840\u001b[0m coords_plus_data \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    841\u001b[0m coords_plus_data[_THIS_ARRAY] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mvariable\n\u001b[0;32m--> 842\u001b[0m dims \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords_plus_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(dims) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims):\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot add coordinates with new dimensions to a DataArray\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    846\u001b[0m     )\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/core/variable.py:3008\u001b[0m, in \u001b[0;36mcalculate_dimensions\u001b[0;34m(variables)\u001b[0m\n\u001b[1;32m   3006\u001b[0m             last_used[dim] \u001b[38;5;241m=\u001b[39m k\n\u001b[1;32m   3007\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m dims[dim] \u001b[38;5;241m!=\u001b[39m size:\n\u001b[0;32m-> 3008\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3009\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflicting sizes for dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3010\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m and length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims[dim]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_used\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3011\u001b[0m             )\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dims\n",
      "\u001b[0;31mValueError\u001b[0m: conflicting sizes for dimension 'time': length 1965 on <this-array> and length 1872 on {'lat': 'lat', 'lon': 'lon', 'time': 'time', 'ensemble': <this-array>}"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ens = 'GHG.'  # GHG ensemble \n",
    "file_suffix= '.185001-200512.nc'\n",
    "ds = [] # initialise array:\n",
    "for member in tqdm(range(1,2)):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).PSL.squeeze()\n",
    "    ds.append(member)\n",
    "for member in tqdm(range(3,4+1)): #add in 4th ensmble when permission is not denied \n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).PSL.squeeze()\n",
    "    ds.append(member)\n",
    "for member in tqdm(range(1,3+1)):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ model +ens +'00' +file).PSL.squeeze()\n",
    "    ds.append(member)\n",
    "post_index_ghg = (xr.concat(ds, dim='ensemble')/100).assign_coords({'time':(da.time)}) #create ensemble dimension after reading in all the ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c5492f5-9962-4147-926a-2287b47099f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.06it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.35 s, sys: 463 ms, total: 2.81 s\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ens = 'LULC.'  #Lulc ensemble \n",
    "file_suffix= '.169001-200512.nc'\n",
    "ds = []\n",
    "for member in tqdm(range(1,3+1)):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(time = slice('1850-02-01', '2006-01-01')).PSL.squeeze()\n",
    "    ds.append(member)\n",
    "\n",
    "ens = 'LULC_HurttPongratz.'\n",
    "file_suffix= '.185001-200512.nc'\n",
    "for member in tqdm(range(1,3+1)):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ model+ ens + '00'+file).sel(time = slice('1850-02-01', '2006-01-01')).PSL.squeeze()\n",
    "    ds.append(member)\n",
    "    \n",
    "post_index_lulc= (xr.concat(ds, dim='ensemble')/100).assign_coords({'time':(da.time)}) #create ensemble dimension after reading in all the ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4c0dcf1-3d23-4972-b31a-b5db96deac64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.5 s, sys: 665 ms, total: 4.17 s\n",
      "Wall time: 9.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ens = 'O3AER.'\n",
    "file_suffix= '.185001-200512.nc'\n",
    "ds = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).PSL.squeeze()\n",
    "    ds.append(member)\n",
    "\n",
    "ens = 'OZONE_AER.'\n",
    "for member in tqdm(range(1,5+1)):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ model+ ens + '00'+file).PSL.squeeze()\n",
    "    ds.append(member)\n",
    "\n",
    "post_index_oza = (xr.concat(ds, dim='ensemble')/100).assign_coords({'time':(da.time)}) #create ensemble dimension after reading in all the ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8758b70-e776-4fd8-99c2-a732eb1bb7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "icntl_index = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/PSL/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.PSL.065001-084912.nc').PSL.squeeze().assign_coords({'time':(i_cntlda.time)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d260ff-8766-49f1-bd4e-753ab6df0ca2",
   "metadata": {},
   "source": [
    "# Read in Second Variable: dD Files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e7cf3-2055-4127-b453-e90ec7c5ba7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate dD for full forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9a134d7-92eb-448c-b37e-19c7219fa11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 8.79 s, total: 2min 11s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ens = ''\n",
    "file_suffix= '.185001-200512.nc'\n",
    "\n",
    "var = 'H218Or'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218Or.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218Or.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H218Or = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H218OL'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OL.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OL.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H218OL = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H218OS'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OS.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OS.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H218OS = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H218OR'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OR.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OR.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H218OR = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H218Os'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218Os.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218Os.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H218Os = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H218OV'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OV.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OV.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H218OV = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H218OI'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OI.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OI.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H218OI = xr.concat(dsf, dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6c5e627-d577-4040-96e0-3702e4d7ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_H218O = post_H218Or + post_H218OR +post_H218Os +post_H218OS + post_H218OI + post_H218OL +post_H218OV # adll all 18O componen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3b5489b-a03c-4561-8aa2-42e0933c58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del post_H218Or \n",
    "del post_H218OR \n",
    "del post_H218Os \n",
    "del post_H218OS \n",
    "del post_H218OI \n",
    "del post_H218OL\n",
    "del post_H218OV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b929161-c292-4825-8c28-ea58aa986633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 8.81 s, total: 2min 11s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ens = ''\n",
    "file_suffix= '.185001-200512.nc'\n",
    "\n",
    "var = 'H216Or'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216Or.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216Or.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H216Or = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H216OL'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OL.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OL.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H216OL = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H216OS'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OS.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OS.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H216OS = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H216OR'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OR.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OR.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H216OR = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H216Os'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216Os.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216Os.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H216Os = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H216OV'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OV.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OV.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H216OV = xr.concat(dsf, dim='ensemble')\n",
    "\n",
    "var = 'H216OI'\n",
    "dsf = []# initialise array:\n",
    "for member in range(1,2):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model1+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OI.squeeze()\n",
    "    dsf.append(member)   \n",
    "for member in range(2,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OI.squeeze()\n",
    "    dsf.append(member)     \n",
    "post_H216OI = xr.concat(dsf, dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35ff5cc7-88e0-4ced-9fbe-ab306bdee841",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_H216O= post_H216Or + post_H216OR +post_H216Os +post_H216OS + post_H216OI + post_H216OL + post_H216OV # add all 16O componets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "377eb721-f0ee-4b90-87db-d6b858d78154",
   "metadata": {},
   "outputs": [],
   "source": [
    "del post_H216Or\n",
    "del post_H216OR \n",
    "del post_H216Os \n",
    "del post_H216OS \n",
    "del post_H216OI \n",
    "del post_H216OL \n",
    "del post_H216OV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddbc48d7-a890-4d83-9450-53c47b2f2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_full =  (((post_H218O/post_H216O)-1)*1000).assign_coords({'time':(da.time)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de891a69-b560-4e51-b2f3-69675ead58eb",
   "metadata": {},
   "source": [
    "## Read in GHG files and Calculate dD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9dca5df-1ec3-4a6f-ba08-93f7e40e611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 18s, sys: 1min 24s, total: 7min 43s\n",
      "Wall time: 7min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "## GHG\n",
    "ens = 'GHG.'  #set ensemble \n",
    "\n",
    "var = 'H218OL'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OL.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H218OL_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H218Or'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218Or.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H218Or_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H218OS'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OS.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H218OS_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H218OR'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OR.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H218OR_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H218Os'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218Os.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H218Os_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H218OV'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OV.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H218OV_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H218OI'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OI.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H218OI_ghg = xr.concat(dsg, dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09080d74-2f09-43e8-a6b1-93b691689cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.1 s, sys: 1min 8s, total: 1min 32s\n",
      "Wall time: 5min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "post_H218O_ghg = post_H218Or_ghg + post_H218OR_ghg +post_H218Os_ghg +post_H218OS_ghg + post_H218OI_ghg + post_H218OL_ghg +post_H218OV_ghg # adll all 18O componen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e70d7dd7-73ba-404e-b09a-b524536755ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "del post_H218Or_ghg \n",
    "del post_H218OR_ghg \n",
    "del post_H218Os_ghg  \n",
    "del post_H218OS_ghg  \n",
    "del post_H218OI_ghg  \n",
    "del post_H218OL_ghg \n",
    "del post_H218OV_ghg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af5ba30c-2d35-4667-9326-9f4ec696136d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 21s, sys: 3min 14s, total: 9min 36s\n",
      "Wall time: 9min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## GHG\n",
    "var = 'H216OL'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OL.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H216OL_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H216Or'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216Or.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H216Or_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H216OS'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OS.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H216OS_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H216OR'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OR.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H216OR_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H216Os'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216Os.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H216Os_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H216OV'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OV.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H216OV_ghg = xr.concat(dsg, dim='ensemble')\n",
    "\n",
    "var = 'H216OI'\n",
    "dsg = []\n",
    "for member in (1,3,4):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model +ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OI.squeeze()\n",
    "    dsg.append(member)\n",
    "post_H216OI_ghg = xr.concat(dsg, dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ceea734-ae01-4c42-ad86-42220908d70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.5 s, sys: 3min 37s, total: 4min\n",
      "Wall time: 6min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "post_H216O_ghg = post_H216Or_ghg + post_H216OR_ghg +post_H216Os_ghg +post_H216OS_ghg + post_H216OI_ghg + post_H216OL_ghg +post_H216OV_ghg # add all 16O componets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f66a85e0-3c46-430f-af1a-4a7783655dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "del post_H216Or_ghg # rC is convective rain \n",
    "del post_H216OR_ghg # L - large scale \n",
    "del post_H216Os_ghg #SC = snow rates convective?\n",
    "del post_H216OS_ghg # SL = snow rates large scale?\n",
    "del post_H216OI_ghg #ice \n",
    "del post_H216OL_ghg #liquid \n",
    "del post_H216OV_ghg #vapor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "725c54e8-81c1-4127-b56f-6d0c4297e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.08 s, sys: 14.9 s, total: 23 s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "var_ghg =  ((((post_H218O_ghg/post_H216O_ghg)-1)*1000).sel(time = slice('1850-02-01', '2006-01-01'))).assign_coords({'time':(da.time)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d485340-d299-423d-b723-e888fbaaf089",
   "metadata": {},
   "source": [
    "## Read in OZA files and Calculate dD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bccf6cbf-2974-4492-98b0-13b8630fe3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 12.2 s, total: 2min 56s\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# OZA\n",
    "ens = 'O3AER.'\n",
    "var = 'H218Or'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218Or.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H218Or_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H218OL'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OL.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H218OL_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H218OS'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OS.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H218OS_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H218OR'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OR.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H218OR_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H218Os'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218Os.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H218Os_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H218OV'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OV.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H218OV_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H218OI'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H218OI.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H218OI_oza = xr.concat(dso, dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64d8789e-1e1c-4114-9e37-53db70564192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.45 s, sys: 2.94 s, total: 6.39 s\n",
      "Wall time: 6.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "post_H218O_oza = post_H218Or_oza + post_H218OR_oza +post_H218Os_oza +post_H218OS_oza+ post_H218OI_oza + post_H218OL_oza+post_H218OV_oza # adll all 18O componen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "925e4f34-f020-42fc-aae5-a1e1e209bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del post_H218Or_oza \n",
    "del post_H218OR_oza \n",
    "del post_H218Os_oza  \n",
    "del post_H218OS_oza  \n",
    "del post_H218OI_oza  \n",
    "del post_H218OL_oza \n",
    "del post_H218OV_oza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b198321-6f84-4ba9-a7e8-3dea20f67cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 43s, sys: 12.4 s, total: 2min 56s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# OZA\n",
    "var = 'H216Or'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216Or.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H216Or_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H216OL'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OL.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H216OL_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H216OS'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OS.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H216OS_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H216OR'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OR.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H216OR_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H216Os'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216Os.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H216Os_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H216OV'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OV.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H216OV_oza = xr.concat(dso, dim='ensemble')\n",
    "\n",
    "var = 'H216OI'\n",
    "dso = []\n",
    "for member in (1,3,4,5):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm + var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290)).H216OI.squeeze()\n",
    "    dso.append(member)   \n",
    "post_H216OI_oza = xr.concat(dso, dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "748a7f46-8d1d-4d8d-a085-8cda72f784ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.45 s, sys: 2.93 s, total: 6.38 s\n",
      "Wall time: 6.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "H216O_oza = post_H216Or_oza + post_H216OR_oza +post_H216Os_oza +post_H216OS_oza +post_H216OI_oza + post_H216OL_oza +post_H216OV_oza # add all 16O componets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8527897a-d4c7-4cbe-89a8-2d655a572caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del post_H216Or_oza # rC is convective rain \n",
    "del post_H216OR_oza # L - large scale \n",
    "del post_H216Os_oza #SC = snow rates convective?\n",
    "del post_H216OS_oza # SL = snow rates large scale?\n",
    "del post_H216OI_oza #ice \n",
    "del post_H216OL_oza #liquid \n",
    "del post_H216OV_oza #vapor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be5559c8-dd15-4c6f-9886-3619d9ff2225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 1.4 s, total: 2.64 s\n",
      "Wall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "var_oza =  ((((post_H218O_oza/H216O_oza)-1)*1000).sel(time = slice('1850-02-01', '2006-01-01'))).assign_coords({'time':(da.time)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f35865-fb20-422d-a1f2-65ddc9b031ab",
   "metadata": {},
   "source": [
    "## Read in LULC files and Calculate dD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7eb21966-6651-43e4-a738-899ae4fd6d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 9.33 s, total: 2min 12s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ens = 'LULC.'  #Lulc ensemble  \n",
    "file_suffix= '.169001-200512.nc'\n",
    "\n",
    "var = 'H218OI'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H218OI.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H218OI_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "\n",
    "var = 'H218Or'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H218Or.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H218Or_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "var = 'H218OS'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H218OS.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H218OS_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "var = 'H218OR' \n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H218OR.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H218OR_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "\n",
    "var = 'H218Os'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H218Os.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H218Os_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "var = 'H218OV'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H218OV.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H218OV_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "var = 'H218OL'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H218OL.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H218OL_lulc = xr.concat(dsl, dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "201b0b84-f572-46b5-80cd-c81a6bbd8d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 s, sys: 2.18 s, total: 4.74 s\n",
      "Wall time: 4.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "post_H218O_lulc = post_H218Or_lulc + post_H218OR_lulc +post_H218Os_lulc +post_H218OS_lulc+ post_H218OI_lulc + post_H218OL_lulc+post_H218OV_lulc # adll all 18O componen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "52f45efd-df33-43c8-9960-61d6fde20495",
   "metadata": {},
   "outputs": [],
   "source": [
    "del post_H218Or_lulc \n",
    "del post_H218OR_lulc \n",
    "del post_H218Os_lulc  \n",
    "del post_H218OS_lulc  \n",
    "del post_H218OI_lulc  \n",
    "del post_H218OL_lulc \n",
    "del post_H218OV_lulc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ee672bca-643e-486b-9fa5-6ade4587d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'H216OI'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H216OI.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H216OI_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "\n",
    "var = 'H216Or'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H216Or.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H216Or_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "var = 'H216OS'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H216OS.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H216OS_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "var = 'H216OR' \n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H216OR.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H216OR_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "\n",
    "var = 'H216Os'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H216Os.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H216Os_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "var = 'H216OV'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H216OV.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H216OV_lulc = xr.concat(dsl, dim='ensemble')\n",
    "\n",
    "var = 'H216OL'\n",
    "dsl = []\n",
    "for member in range(1,3+1):\n",
    "    id = str(member)\n",
    "    file = id+file_prefix+var+ file_suffix\n",
    "    member = xr.open_dataset(basedir_atm+ var+ i_model+ ens + '00'+file).sel(lat=slice(-35,35),lon=slice(75,290), time = slice('1850-02-01', '2006-01-01')).H216OL.squeeze()\n",
    "    dsl.append(member)\n",
    "post_H216OL_lulc = xr.concat(dsl, dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dcacc602-fc66-4d08-805e-fd51c2ca5961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.63 s, sys: 2.15 s, total: 4.79 s\n",
      "Wall time: 4.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "post_H216O_lulc = post_H216Or_lulc + post_H216OR_lulc +post_H216Os_lulc +post_H216OS_lulc+ post_H216OI_lulc + post_H216OL_lulc+post_H216OV_lulc # adll all 16O componen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b8087009-71ea-4e9b-ba87-eeafa9c127fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del post_H216Or_lulc \n",
    "del post_H216OR_lulc \n",
    "del post_H216Os_lulc  \n",
    "del post_H216OS_lulc  \n",
    "del post_H216OI_lulc  \n",
    "del post_H216OL_lulc \n",
    "del post_H216OV_lulc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3083ee93-5b1c-49e9-96bb-960fcfe54544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 846 ms, sys: 1.16 s, total: 2.01 s\n",
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "var_lulc =  ((((post_H218O_lulc/post_H216O_lulc)-1)*1000).sel(time = slice('1850-02-01', '2006-01-01'))).assign_coords({'time':(da.time)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceaf4d8-3f3b-42cf-90d9-fd1a3b46c5ef",
   "metadata": {},
   "source": [
    "## Read in Control files and Calculate dD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dce58b82-5b8a-42b6-8e34-6754a77e836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Control \n",
    "##cntl_PREC_H218O = cntl_PRECRC_H218Or + cntl_PRECRL_H218OR +cntl_PRECSC_H218Os +cntl_PRECSL_H218OS\n",
    "#cntl_PREC_H216O = cntl_PRECRC_H216Or + cntl_PRECRL_H216OR +cntl_PRECSC_H216Os +cntl_PRECSL_H216OS\n",
    "#cntl_var = (((cntl_PREC_H218O/cntl_PREC_H216O)-1)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8df84b90-9bc5-4c02-b012-cd14263fa315",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dH Control \n",
    "cntl_H218Or = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H218Or/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H218Or.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H218Or.squeeze() #AttributeError: 'CFTimeIndex' object has no attribute '_cache'\n",
    "cntl_H216Or = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H216Or/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H216Or.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H216Or.squeeze()\n",
    "\n",
    "cntl_H218OR = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H218OR/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H218OR.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H218OR.squeeze() #AttributeError: 'CFTimeIndex' object has no attribute '_cache'\n",
    "cntl_H216OR = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H216OR/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H216OR.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H216OR.squeeze()\n",
    "\n",
    "cntl_H218Os = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H218Os/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H218Os.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H218Os.squeeze() #AttributeError: 'CFTimeIndex' object has no attribute '_cache'\n",
    "cntl_H216Os = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H216Os/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H216Os.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H216Os.squeeze()\n",
    "\n",
    "cntl_H218OS = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H218OS/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H218OS.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H218OS.squeeze() #AttributeError: 'CFTimeIndex' object has no attribute '_cache'\n",
    "cntl_H216OS = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H216OS/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H216OS.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H216OS.squeeze()\n",
    "\n",
    "cntl_H218OI = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H218OI/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H218OI.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H218OI.squeeze() #AttributeError: 'CFTimeIndex' object has no attribute '_cache'\n",
    "cntl_H216OI = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H216OI/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H216OI.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H216OI.squeeze()\n",
    "\n",
    "cntl_H218OL = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H218OL/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H218OL.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H218OL.squeeze() #AttributeError: 'CFTimeIndex' object has no attribute '_cache'\n",
    "cntl_H216OL = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H216OL/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H216OL.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H216OL.squeeze()\n",
    "\n",
    "cntl_H218OV = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H218OV/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H218OV.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H218OV.squeeze() #AttributeError: 'CFTimeIndex' object has no attribute '_cache'\n",
    "cntl_H216OV = xr.open_dataset('/glade/campaign/cesm/collections/cesmLME/CESM-CAM5-LME/atm/proc/tseries/monthly/H216OV/b.ie12.B1850CN.f19_g16.850cntl.001.cam.h0.H216OV.065001-084912.nc').sel(lat=slice(-35,35),lon=slice(75,290)).H216OV.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7e6224b1-769d-45d5-aabf-c643a9da88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Control \n",
    "cntl_H218O = cntl_H218Or + cntl_H218OR +cntl_H218Os +cntl_H218OS +cntl_H218OI +cntl_H218OL +cntl_H218OV\n",
    "cntl_H216O = cntl_H216Or + cntl_H216OR +cntl_H216Os +cntl_H216OS +cntl_H216OI +cntl_H216OL +cntl_H216OV\n",
    "var_cntl = (((cntl_H218O/cntl_H216O)-1)*1000).assign_coords({'time':(i_cntlda.time)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "045bb6e4-8c70-446a-bf6f-1384f29da504",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "NetCDF: HDF error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvar_cntl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvar_cntl.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/core/dataarray.py:4090\u001b[0m, in \u001b[0;36mDataArray.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   4086\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4087\u001b[0m     \u001b[38;5;66;03m# No problems with the name - so we're fine!\u001b[39;00m\n\u001b[1;32m   4088\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dataset()\n\u001b[0;32m-> 4090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   4091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4094\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4098\u001b[0m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4101\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/backends/api.py:1315\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;66;03m# TODO: figure out how to refactor this logic (here and in save_mfdataset)\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# to avoid this mess of conditionals\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;66;03m# TODO: allow this work (setting up the file for writing array data)\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# to be parallelized with dask\u001b[39;00m\n\u001b[0;32m-> 1315\u001b[0m     \u001b[43mdump_to_store\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoclose:\n\u001b[1;32m   1319\u001b[0m         store\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/backends/api.py:1362\u001b[0m, in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder:\n\u001b[1;32m   1360\u001b[0m     variables, attrs \u001b[38;5;241m=\u001b[39m encoder(variables, attrs)\n\u001b[0;32m-> 1362\u001b[0m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/backends/common.py:356\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.store\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_attributes(attributes)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_dimensions(variables, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims)\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_variables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_encoding_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/backends/common.py:398\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.set_variables\u001b[0;34m(self, variables, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    393\u001b[0m check \u001b[38;5;241m=\u001b[39m vn \u001b[38;5;129;01min\u001b[39;00m check_encoding_set\n\u001b[1;32m    394\u001b[0m target, source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_variable(\n\u001b[1;32m    395\u001b[0m     name, v, check, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims\n\u001b[1;32m    396\u001b[0m )\n\u001b[0;32m--> 398\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/backends/common.py:243\u001b[0m, in \u001b[0;36mArrayWriter.add\u001b[0;34m(self, source, target, region)\u001b[0m\n\u001b[1;32m    241\u001b[0m     target[region] \u001b[38;5;241m=\u001b[39m source\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m source\n",
      "File \u001b[0;32m/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:79\u001b[0m, in \u001b[0;36mBaseNetCDF4Array.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m     78\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mautoclose:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mclose(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:5519\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Variable.__setitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:5802\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Variable._put\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2034\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NetCDF: HDF error"
     ]
    }
   ],
   "source": [
    "var_cntl.to_netcdf('dd'+'var_cntl.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c35c1e9c-979f-4231-97f7-96b8e36bf17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cntl_H218Or \n",
    "del cntl_H216Or \n",
    "del cntl_H218OR \n",
    "del cntl_H216OR \n",
    "del cntl_H218Os \n",
    "del cntl_H216Os \n",
    "del cntl_H218OS\n",
    "del cntl_H216OS \n",
    "del cntl_H218OI \n",
    "del cntl_H216OI \n",
    "del cntl_H218OL\n",
    "del cntl_H216OL \n",
    "del cntl_H218OV \n",
    "del cntl_H216OV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93a5ee-d994-4fa9-b09d-e9906e58d0c6",
   "metadata": {},
   "source": [
    "# Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "26fd9dbf-0608-46d1-87e2-38c9916bd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_mer(r):\n",
    "    r_lat= r.sel(lat=slice(-5, 5)).mean(dim='lat')\n",
    "    r_mer =  r_lat.sel(lon=slice(110, 270))\n",
    "    r_mer18 = r_mer.isel(lev = 18)\n",
    "    return r_mer, r_mer18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96811603-d1a4-46f7-9734-1390131a5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Anomaly relative to Climatology for variable \n",
    "def clim_anom(v, p): \n",
    "    units = v/100 #convert to hPa d18O #convert precip 1000*24*3600\n",
    "    units_p = p/100 #convert to hPa d18O #convert precip 1000*24*3600\n",
    "   \n",
    "    pi = units_p.assign_coords({'time':(i_cntlda.time)}) #convert time dimestion to have 01 be january \n",
    "    vv = units.assign_coords({'time':(da.time)}) #convert time dimestion to have 01 be january \n",
    "\n",
    "    del units_p\n",
    "    del units\n",
    "\n",
    "    # Group by month dimension\n",
    "    pi_month =pi.groupby('time.month').mean('time') #calculate monthly climatology\n",
    "    clim_month = vv.groupby('time.month').mean('time') #calculate monthly climatology #return\n",
    "\n",
    "    # Calcualte anomaly relative to climatology \n",
    "    pi_anom = pi.groupby('time.month') - pi_month # 1 calculate the anomaly relative to the climatology x4\n",
    "    clim_anom = vv.groupby('time.month') - clim_month #2 calculate the anomaly relative to the climatology x4\n",
    "    \n",
    "    del pi_month\n",
    "    del clim_month \n",
    "\n",
    "    # Ensemble average effect of forcing \n",
    "    eforcing_avg = (vv - pi).mean(dim = 'ensemble') # 3netcdf x4\n",
    "    # Meridtrional Average \n",
    "    clim_mer, clim_mer18 = reg_mer(vv) # x4\n",
    "    pi_mer, pi_mer18 = reg_mer(pi) #x1\n",
    "\n",
    "    #del vv\n",
    "    #del pi \n",
    "    # Ensemble averagefor climatigy realtive to anomaly and effect of forcing anomalies \n",
    "    anom_eforcing_avg = (clim_anom - pi_anom).mean(dim = 'ensemble') #4 return  #netcdfx4\n",
    "    clim_anomavg = clim_anom.mean(dim = 'ensemble')  #5 return x4\n",
    "    \n",
    "    # Vertical Slice \n",
    "    clim_anom_mer, clim_anom_mer18 = reg_mer(clim_anom) #6, 7 x4\n",
    "    pi_anom_mer, pi_anom_mer18 = reg_mer(pi_anom) #8, 9 x1\n",
    "    \n",
    "    # all levels  tropical domain  effect of forcing climatology and climatology anomaly \n",
    "    eforcing_mer = (clim_mer - pi_mer).mean(dim = 'ensemble') # 10 return x4\n",
    "    anom_eforcing_mer = (clim_anom_mer - pi_anom_mer).mean(dim = 'ensemble') # 11 return x4\n",
    "\n",
    "    del clim_mer\n",
    "    del pi_mer\n",
    "\n",
    "    # level 18 effect of forcing climatology and climatology anomaly \n",
    "    \n",
    "    eforcing_mer18 = (clim_mer18 - pi_mer18).mean(dim = 'ensemble') #12 return x4\n",
    "    anom_eforcing_mer18 = (clim_anom_mer18 - pi_anom_mer18).mean(dim = 'ensemble') # 13 return x4\n",
    "\n",
    "  \n",
    "    return vv, pi, clim_anom, pi_anom, eforcing_avg, anom_eforcing_avg,clim_anomavg, anom_eforcing_mer, eforcing_mer, anom_eforcing_mer,anom_eforcing_mer18 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8cde2-a337-4d63-93b5-c625a81def12",
   "metadata": {},
   "source": [
    "# Select Atmospheric Pressure Level (524 hpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "472dc79f-1d51-473d-9bf7-9ddf29342119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#var_ghg = var_ghg.isel(lev = 18)\n",
    "#var_full = var_full.isel(lev = 18)\n",
    "#var_oza = var_oza.isel(lev = 18)\n",
    "#var_lulc = var_full.isel(lev = 18)\n",
    "#var_cntl = var_cntl.isel(lev = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe5dee-d2c8-48da-b152-63b0208f5080",
   "metadata": {},
   "source": [
    "# Calculate Climatology \n",
    "- Convert CESM data to start at 1 January, not python index\n",
    "- Weight by grid cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "57eeb588-9aed-436c-87e0-e5460018a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'dD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d2120210-dca8-46ae-854c-d10394060c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Anomaly relative to Climatology for variable \n",
    "def clim(c): \n",
    "    clim_var = c.groupby('time.month').mean('time') #calculate monthly climatology\n",
    "    clim_anom = c.groupby('time.month') - clim_var # calculate the anomaly relative to the climatology \n",
    "    #anom_weights = weights(clim_anom) #weight functino to account for lat grid size \n",
    "    return  clim_anom #return the anomaly calcuated by the anomaly with time and lat conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d45d431a-db2b-4018-be44-34c372410926",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_full.to_netcdf(variable+'full.nc')\n",
    "var_ghg.to_netcdf(variable+'ghg.nc')\n",
    "var_oza.to_netcdf(variable+'oza.nc')\n",
    "var_lulc.to_netcdf(variable+'lulc.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ee37c781-a1e1-4e09-a1cc-644e700f5e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/glade/u/home/calipfleger/stats/PWC_Paper'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7991cda9-978a-4233-aefa-29ed3af0b7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.1 s, sys: 27.7 s, total: 1min 16s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate anomaly relative to climatology for global variable forcing files \n",
    "anom_full = clim(var_full)#var_full.isel(lev = 18))\n",
    "anom_ghg = clim(var_ghg)\n",
    "anom_lulc = clim(var_lulc)\n",
    "anom_oza = clim(var_oza)\n",
    "anom_cntl = clim_cntl(var_cntl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4ee54aac-f93f-471b-8c41-9dc6381fadd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del var_full \n",
    "#del var_ghg\n",
    "#del var_oza\n",
    "#del var_lulc\n",
    "#del var_cntl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62254ff6-7b35-4621-b282-e5d06eed026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate anomaly relative to climatology for index forcing files \n",
    "anom_index = clim(post_index)\n",
    "anom_index_ghg = clim(post_index_ghg)\n",
    "anom_index_lulc = clim(post_index_lulc)\n",
    "anom_index_oza = clim(post_index_oza)\n",
    "anom_index_cntl = clim_cntl(icntl_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ffaae2-c5da-4c34-bfdb-25c792499b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del post_index\n",
    "#del post_index_ghg\n",
    "#del post_index_lulc\n",
    "#del post_index_oza\n",
    "#del icntl_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79b810-3a35-48b9-b722-be17e3fac1a4",
   "metadata": {},
   "source": [
    "# Mapping the Climatological Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f22e66-94d8-44b2-bf9b-c797b16ad446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tens (tt):\n",
    "    t18 = tt.isel(lev = 18)\n",
    "    test = t18.mean(dim = 'time')\n",
    "    test2 = test.mean(dim = 'ensemble')\n",
    "    return test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe99a7c-e45f-4a90-9322-d757a43c2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate anomaly relative to climatology for global variable forcing files \n",
    "anom_var_avg = tens(anom_full)#var_full.isel(lev = 18))\n",
    "anom_var_ghg_avg  = tens(anom_ghg)\n",
    "anom_var_lulc_avg = tens(anom_lulc)\n",
    "anom_var_oza_avg  = tens(anom_oza)\n",
    "#anom_var_cntl_avg  = tens(anom_var_cntl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47156a07-9fce-4e37-94d8-73787a1a1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_maps = [anom_var_avg,anom_var_ghg_avg, anom_var_oza_avg ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301878e-da8e-4680-b39d-b0d359b93f3b",
   "metadata": {},
   "source": [
    "# Vertical Slice: Meridional Mean and Vertical Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea17251-b67e-4ba4-a789-c5632f79a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertplot(v):\n",
    "    # Plotting the average of the meridional mean of the vertical velocity and pressure contour lines\n",
    "    ax = fig.add_subplot(2,2,i+1)\n",
    "    contour = v[i].plot.contourf(ax=ax, levels=30, cmap='RdBu_r', vmin= -0.000002, vmax= 0.000002)\n",
    "    pressure_contour = v[i].plot.contour(ax=ax, levels=10, colors='k', linewidths=0.5)\n",
    "    # Add contour line labels with numeric values\n",
    "    ax.clabel(pressure_contour, fmt='%1.2f', colors='k', fontsize=8)\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Pressure (hPa)')\n",
    "    plt.title(var_name[i] + ' Average Meridional Mean dH')\n",
    "    #cbar = plt.colorbar(contour, label='Pa/s')  # Create the colorbar using the contour object\n",
    "    #plt.gca().invert_yaxis()  # Flip the y-axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1009b49-23f1-405f-9d46-705d8a4b76b3",
   "metadata": {},
   "source": [
    "# 524 hPa 20-20 avg over 1850-2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512d5bd-0e5f-455f-b858-ab7069b46455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for meridional mean and lat lon range \n",
    "def vert_avg(mm):\n",
    "    #m_we = weights(m)\n",
    "    mer_lat= mm.sel(lat=slice(-5, 5)).mean(dim='lat')\n",
    "    mer_e = mer_lat.sel(lon=slice(180, 260)).mean(dim='lon')\n",
    "    mer_w = mer_lat.sel(lon=slice(130, 150)).mean(dim='lon')\n",
    "    mer_index = (mer_w - mer_e).mean(dim = 'ensemble')\n",
    "    mer_series = mer_index.mean(dim ='time').to_series()\n",
    "    return mer_series, mer_index, mer_e, mer_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f7e8a4-f411-45eb-9590-d9276f96e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for meridional mean and lat lon range \n",
    "def vert_avg(ef):\n",
    "    #m_we = weights(m)\n",
    "    #ef = v - c # m.isel(lev = '18')\n",
    "    mer_e = (ef.sel(lat=slice(-5, 5), lon=slice(180, 260))).mean(('lat','lon'))\n",
    "    mer_w = (ef.sel(lat=slice(-5, 5),lon=slice(130, 150))).mean(('lat','lon'))\n",
    "    mer_index = ((mer_w - mer_e)).mean(dim = 'ensemble')\n",
    "    mer_series = mer_index.mean(dim ='time').to_series()\n",
    "    merea = mer_e.mean(dim = 'ensemble')\n",
    "    e_series = mer_index.mean(dim ='time').to_series()\n",
    "    mer_index = ((mer_w - mer_e)).mean(dim = 'ensemble')\n",
    "    mer_series = mer_index.mean(dim ='time').to_series()\n",
    "    return mer_series, mer_index, eb, mer_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa3a2d-5fb0-462c-a2de-10538b1f03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def east_vert(ev):\n",
    "    dfe = []\n",
    "    tt = ev[i].isel(lat=slice(-5, 5), lon=slice(180, 260)).mean(('lat','lon')).mean(dim = 'ensemble').mean(dim ='time').to_series()\n",
    "    dfe.append(tt)\n",
    "    dfe.plot(kind='bar', title ='dDV Anomalies as a function of height in the atmosphere 5N-5S').invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74253dee-0d05-49d4-968f-4a3d5cae0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for meridional mean and lat lon range \n",
    "def vert_avglev(v,c):\n",
    "    ef = v.isel(lev = 18) - c.isel(lev = 18) # m.isel(lev = '18')\n",
    "    mer_e= ef.sel(lat=slice(-5, 5), lon=slice(180, 260)).mean(('lat,''lon'))\n",
    "    mer_w = ef.sel(lat=slice(-5, 5),lon=slice(130, 150)).mean(('lat,''lon'))\n",
    "    mer_index = (mer_w - mer_e).mean(dim = 'ensemble')\n",
    "    mer_series = mer_index.mean(dim ='time').to_series()\n",
    "    return mer_, mer_index, mer_e, mer_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32d213-dd16-4394-90a5-9344225c68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = [anom_full, anom_ghg, anom_oza, anom_lulc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7065375-5123-48bb-9c32-6e29cc0dbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = anom_full.isel(lat=slice(-5, 5), lon=slice(180, 260)).mean(('lat','lon')).mean(dim = 'ensemble').mean(dim ='time').to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fa59b-f6e5-4714-8fc1-4945ac4495cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(flist)):\n",
    "    test = east_vert(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597dee5-ecb3-45dc-b63f-c40391e0a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "series, series_ghg, series_oza, series_lulc = mer_clim[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94fd58-7b78-419c-bb75-8090c0fc050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "series, index_full, east, west  = vert_avg(anom_full)#var_full.isel(lev = 18))\n",
    "series_ghg, index_ghg, east_ghg, west_ghg  = vert_avg(anom_ghg)\n",
    "series_lulc, index_luzlc, east_lulc, west_lulc  = vert_avg(anom_lulc)\n",
    "series_oza, index_oza, east_oza, west_oza  = vert_avg(anom_oza)\n",
    "#, index, east, west   = mer_clim_cntl(anom_var_cntl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99391da7-db9e-4480-bcff-a6c829d183ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe = pd.DataFrame({'Full Forcing': east, 'GHG': east_ghg, 'Aerosol': east_oza, 'Land Change': east_lulc})\n",
    "dfe.plot(kind='bar', title ='dDV Anomalies as a function of height in the atmosphere 5N-5S').invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42963b82-f608-49ad-bfa8-a02415f86817",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Full Forcing': series, 'GHG': series_ghg, 'Aerosol': series_oza, 'Land Change': series_lulc})\n",
    "df.plot(kind='bar', title ='dDV Anomalies as a function of height in the atmosphere 5N-5S').invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d9505-74a0-4d5c-89e0-400c01c54aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_avg = [series, series_ghg, series_oza, series_lulc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b7bb5-65d3-449e-95ca-882a5404a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trop_plot(p):\n",
    "    ax = fig.add_subplot(2,2,i+1, projection=ccrs.Robinson(central_longitude=180))\n",
    "    ax.coastlines()\n",
    "    plt.contourf(lon,lat,p[i],cmap='bwr', add_colorbar=False, transform=ccrs.PlateCarree())\n",
    "    plt.pcolor(lon,lat,p[i], cmap='bwr', vmin=-1, vmax=1, transform=ccrs.PlateCarree())  \n",
    "    #calculate significance of trends\n",
    "    sig_p=xr.DataArray(data=p[i].values*np.sqrt((df-2)/(1-np.square(p[i].values))), \n",
    "                       dims=[\"lat\",\"lon'\"], coords=[lat, lon])\n",
    "    sig_p.plot.contourf(ax=ax,levels = [-1*t95, -1*t90, t90, t95], colors='none', \n",
    "                        hatches=['..', None, None, None, '..'], extend='both', add_colorbar=False, \n",
    "                        transform=ccrs.PlateCarree())\n",
    "    ax.set_extent([75, 290, -30, 30], crs=ccrs.PlateCarree())\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True)\n",
    "    gl.xlabels_top = False\n",
    "    gl.ylabels_left = True\n",
    "    gl.xlines = False\n",
    "    gl.ylines = False\n",
    "    gl.xlocator = mticker.FixedLocator([-90, -135, 180, 135, 90])\n",
    "    gl.ylocator = mticker.FixedLocator([-20, -10, 0, 10, 20])\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.xlabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    gl.ylabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.ylabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    ax.add_geometries([Ering], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth=2.5)\n",
    "    ax.add_geometries([Wring], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth =2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.title(var_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6f5510-05b1-4021-9d94-cf4b7f2a7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "fig.dpi = 600\n",
    "for i in range(len(clim_maps)):\n",
    "    vertplot(clim_maps)\n",
    "plt.suptitle('dD Correlation with ' + region, y =.65)\n",
    "#plt.rcParams['font.size'] = '24'\n",
    "plt.subplots_adjust(hspace=-1, wspace=0)\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
    "plt.tight_layout()\n",
    "plt.savefig(variable + \"glob_cor_post_anom.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e95ad1-834d-4c76-b314-126e95d94115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for meridional mean and lat lon range \n",
    "def mer_series(m):\n",
    "    #m_we = weights(m)\n",
    "    c = m.assign_coords({'time':(da.time)}) #convert time dimestion to have 01 be january \n",
    "    mer_lat= c.sel(lat=slice(-20, 20)).mean(dim='lat')\n",
    "    mer_lon = mer_lat.sel(lon=slice(110, 270)).mean(dim='lon')\n",
    "    #mer_clim = clim(mer_lon)\n",
    "    mer_time = mer_lon.mean(dim='time')\n",
    "    mer_avg = mer_time.mean(dim = 'ensemble')\n",
    "    mer_series = mer_avg.to_series()\n",
    "    return mer_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26d032-8859-4703-9f93-b8842b026cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for meridional mean and lat lon range \n",
    "def mer_series_cntl(m):\n",
    "    c = m.assign_coords({'time':(i_cntlda.time)}) #convert time dimestion to have 01 be january \n",
    "    mer_lat= c.sel(lat=slice(-20, 20)).mean(dim='lat')\n",
    "    mer_lon = mer_lat.sel(lon=slice(110, 270)).mean(dim='lon')\n",
    "    mer_time = mer_lon.mean(dim='time')\n",
    "    mer_series = mer_time.to_series()\n",
    "    return mer_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a8cf7-f055-4ab2-bdc6-6d4723571645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate anomaly relative to climatology for global variable forcing files \n",
    "series = mer_series(anom_var)#var_full.isel(lev = 18))\n",
    "series_ghg  = mer_series(anom_var_ghg)\n",
    "series_lulc = mer_series(anom_var_lulc)\n",
    "series_oza = mer_series(anom_var_oza)\n",
    "#series_cntl = mer_cntl(anom_var_cntl)\n",
    "df = pd.DataFrame({'Full': series, 'GHG': series_ghg, 'OZA': series_oza})\n",
    "df.plot(kind='bar', title ='dDV Anomalies as a function of height in the atmosphere 20N-20S').invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf287824-0a2c-43a4-a3e6-da62999d5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "series_cntl  = mer_cntl(var_cntl)\n",
    "series = mer_series(var_full)#var_full.isel(lev = 18))\n",
    "series_ghg  = mer_series(var_ghg)\n",
    "#anom_var_lulc = clim(var_lulc)\n",
    "series_oza = mer_series(var_oza)\n",
    "\n",
    "df = pd.DataFrame({'Full': series, 'GHG': series_ghg, 'OZA': series_oza})\n",
    "df.plot(kind='bar', title ='dD as a function of height in the atmosphere 20N-20S').invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976bc7c1-4bea-4a57-b9b3-fa992b8e4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "series =series - series_cntl\n",
    "series_ghg =series_ghg - series_cntl\n",
    "series_oza =series_oza - series_cntl\n",
    "\n",
    "df = pd.DataFrame({'Full': series, 'GHG': series_ghg, 'OZA': series_oza})\n",
    "df.plot(kind='bar', title ='dD as a function of height in the atmosphere 20N-20S \\n (Forcing - PI CNTL)').invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37b982-a0e5-48b6-8931-9e1048bcc953",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Calculate PWC index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d28c09-3a29-4845-9464-35ffef41ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index = pwci(anom_index)\n",
    "index_ghg = pwci(anom_index_ghg)\n",
    "index_lulc = pwci(anom_index_lulc)\n",
    "index_oza = pwci(anom_index_oza)\n",
    "index_cntl = pwci(anom_index_cntl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef44b76-bf8d-4640-beef-a44258ee9cd2",
   "metadata": {},
   "source": [
    "# Calculate Correlation with PWC Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23950f11-045a-4099-9cd9-8e97dd875a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_f = cor(anom_var, index)\n",
    "cor_g = cor(anom_var_ghg, index_ghg)\n",
    "cor_l = cor(anom_var_lulc, pwc_lulc)\n",
    "cor_o = cor(anom_var_oza, index_oza)\n",
    "cors = [cor_f, cor_g, cor_o, cor_var_cntl]#make list of correlations to loop over \n",
    "\n",
    "cor_var_cntl = cor(anom_var_cntl, index_cntl)\n",
    "controls = [cor_var_cntl]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c393001-4406-4ce2-a3f7-94d841c641a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plotting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44877ebf-af86-4922-998c-ca2562be35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = post_index.lat\n",
    "lon = post_index.lon\n",
    "lent = len(cors)\n",
    "var_name = ['Full Forcing', 'GHG','Ozone-Aerosol', 'PI Cntl']#, 'ANTHRO']'LULC'\n",
    "\n",
    "df = 35 #degrees of freedom \n",
    "t90=stats.t.ppf(1-0.05, df-2) # testing for p-value 90% significant \n",
    "t95=stats.t.ppf(1-0.025, df-2) # testing for p-value 95% significant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f3479-298a-46e5-987d-95b77cefe8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertplot(v):\n",
    "    # Plotting the average of the meridional mean of the vertical velocity and pressure contour lines\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    contour = v.plot.contourf(ax=ax, levels=30, cmap='RdBu_r', vmin= -0.000002, vmax= 0.000002)\n",
    "    pressure_contour = v.plot.contour(ax=ax, levels=10, colors='k', linewidths=0.5)\n",
    "    # Add contour line labels with numeric values\n",
    "    ax.clabel(pressure_contour, fmt='%1.2f', colors='k', fontsize=8)\n",
    "\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Pressure (hPa)')\n",
    "    plt.title(var_name + ' Average Meridional Mean dH')\n",
    "    #cbar = plt.colorbar(contour, label='Pa/s')  # Create the colorbar using the contour object\n",
    "    #plt.gca().invert_yaxis()  # Flip the y-axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33abe8fa-a647-453f-891e-816c6951b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trop_plot(p):\n",
    "    ax = fig.add_subplot(2,2,i+1, projection=ccrs.Robinson(central_longitude=180))\n",
    "    ax.coastlines()\n",
    "    plt.contourf(lon,lat,p[i],cmap='bwr', add_colorbar=False, transform=ccrs.PlateCarree())\n",
    "    plt.pcolor(lon,lat,p[i], cmap='bwr', vmin=-1, vmax=1, transform=ccrs.PlateCarree())  \n",
    "    #calculate significance of trends\n",
    "    sig_p=xr.DataArray(data=p[i].values*np.sqrt((df-2)/(1-np.square(p[i].values))), \n",
    "                       dims=[\"lat\",\"lon'\"], coords=[lat, lon])\n",
    "    sig_p.plot.contourf(ax=ax,levels = [-1*t95, -1*t90, t90, t95], colors='none', \n",
    "                        hatches=['..', None, None, None, '..'], extend='both', add_colorbar=False, \n",
    "                        transform=ccrs.PlateCarree())\n",
    "    ax.set_extent([75, 290, -30, 30], crs=ccrs.PlateCarree())\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True)\n",
    "    gl.xlabels_top = False\n",
    "    gl.ylabels_left = True\n",
    "    gl.xlines = False\n",
    "    gl.ylines = False\n",
    "    gl.xlocator = mticker.FixedLocator([-90, -135, 180, 135, 90])\n",
    "    gl.ylocator = mticker.FixedLocator([-20, -10, 0, 10, 20])\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.xlabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    gl.ylabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.ylabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    ax.add_geometries([Ering], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth=2.5)\n",
    "    ax.add_geometries([Wring], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth =2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.title(var_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd3731-65e8-407c-a3e2-887739252e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmap='BrBG' Brown to teal for precip dry/wet \n",
    "def gplot(p):\n",
    "    ax = fig.add_subplot(2,2,i+1, projection=ccrs.Robinson(central_longitude=180))\n",
    "    ax.coastlines()\n",
    "    plt.contourf(lon,lat,p[i],cmap='bwr', add_colorbar=False, transform=ccrs.PlateCarree())\n",
    "    plt.pcolor(lon,lat,p[i], cmap='bwr', vmin=-1, vmax=1, transform=ccrs.PlateCarree())  \n",
    "    #calculate significance of trends\n",
    "    sig_p=xr.DataArray(data=p[i].values*np.sqrt((df-2)/(1-np.square(p[i].values))), \n",
    "                       dims=[\"lat\",\"lon'\"], coords=[lat, lon])\n",
    "    sig_p.plot.contourf(ax=ax,levels = [-1*t95, -1*t90, t90, t95], colors='none', \n",
    "                        hatches=['..', None, None, None, '..'], extend='both', add_colorbar=False, \n",
    "                        transform=ccrs.PlateCarree())\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True)\n",
    "    gl.xlabels_top = False\n",
    "    gl.ylabels_left = True\n",
    "    gl.xlines = False\n",
    "    gl.ylines = False\n",
    "    gl.xlocator = mticker.FixedLocator([-90, -135, 180, 135, 90])\n",
    "    gl.ylocator = mticker.FixedLocator([-90, -60, -30, 0, 30, 60, 90])\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.xlabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    gl.ylabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.ylabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    ax.add_geometries([Ering], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth=2.5)\n",
    "    ax.add_geometries([Wring], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth =2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.title(var_name[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edfd587-a8bc-4f57-a71d-4233288bbeed",
   "metadata": {},
   "source": [
    "# Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e416ee-6d4d-4216-b688-126478c09658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of what to plot\n",
    "controls_in = [cor_in, cor_in, cor_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a491262-de8c-4a52-b99b-95f897b977a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558112eb-34f4-4220-ba8f-17e22bd17faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "fig.dpi = 600\n",
    "for i in range(len(cors)):\n",
    "    trop_plot(cors)\n",
    "plt.suptitle('dD Correlation with ' + region, y =.65)\n",
    "#plt.rcParams['font.size'] = '24'\n",
    "plt.subplots_adjust(hspace=-1, wspace=0)\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
    "plt.tight_layout()\n",
    "plt.savefig(variable + \"glob_cor_post_anom.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce876291-6575-4160-94b7-839f24667793",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "fig.dpi = 600\n",
    "for i in range(len(cors)):\n",
    "    gplot(cors)\n",
    "plt.suptitle('dD Correlation with ' + region, y =.75)\n",
    "#plt.rcParams['font.size'] = '24'\n",
    "plt.subplots_adjust(hspace=0, wspace=0)\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
    "plt.tight_layout()\n",
    "plt.savefig(variable + \"glob_cor_post_anom.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07916fb8-fb04-4aa8-9316-46c9dcf77de4",
   "metadata": {},
   "source": [
    "# Correlation with PWC - PI variable correlation with PI CNTL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce4af4-dd03-470d-b8cc-1838d426b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Map \n",
    "def globcntl_plot(p, pp):\n",
    "    #p = p[i] - cor_PSL[i] # subtract control run correlation \n",
    "    w = p[i] - pp\n",
    "    ax = fig.add_subplot(2,2,i+1, projection=ccrs.Robinson(central_longitude=180))\n",
    "    ax.coastlines()\n",
    "    plt.contourf(lon,lat,w,cmap='bwr', add_colorbar=False, transform=ccrs.PlateCarree())\n",
    "    plt.pcolor(lon,lat,w, cmap='bwr', vmin=-0.2, vmax=0.2, transform=ccrs.PlateCarree())  \n",
    "    #calculate significance of trends\n",
    "    sig_p=xr.DataArray(data=p[i].values*np.sqrt((df-2)/(1-np.square(p[i].values))), \n",
    "                       dims=[\"lat\",\"lon'\"], coords=[lat, lon])\n",
    "    sig_p.plot.contourf(ax=ax,levels = [-1*t95, -1*t90, t90, t95], colors='none', \n",
    "                        hatches=['..', None, None, None, '..'], extend='both', add_colorbar=False, \n",
    "                        transform=ccrs.PlateCarree())\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True)\n",
    "    gl.xlabels_top = False\n",
    "    gl.ylabels_left = True\n",
    "    gl.xlines = False\n",
    "    gl.ylines = False\n",
    "    gl.xlocator = mticker.FixedLocator([-90, -135, 180, 135, 90])\n",
    "    gl.ylocator = mticker.FixedLocator([-90, -60, -30, 0, 30, 60, 90])\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.xlabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    gl.ylabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.ylabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    ax.add_geometries([Ering], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth=2.5)\n",
    "    ax.add_geometries([Wring], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth =2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.title(var_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123b4bc-dd0f-4846-ad58-04e5a88c3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Map \n",
    "def tropcntl_plot(p, pp):\n",
    "    #p = p[i] - cor_PSL[i] # subtract control run correlation \n",
    "    w = p[i] - pp\n",
    "    ax = fig.add_subplot(2,2,i+1, projection=ccrs.Robinson(central_longitude=180))\n",
    "    ax.coastlines()\n",
    "    plt.contourf(lon,lat,w,cmap='bwr', add_colorbar=False, transform=ccrs.PlateCarree())\n",
    "    plt.pcolor(lon,lat,w, cmap='bwr', vmin=-0.2, vmax=0.2, transform=ccrs.PlateCarree())  \n",
    "    #calculate significance of trends\n",
    "    sig_p=xr.DataArray(data=p[i].values*np.sqrt((df-2)/(1-np.square(p[i].values))), \n",
    "                       dims=[\"lat\",\"lon'\"], coords=[lat, lon])\n",
    "    sig_p.plot.contourf(ax=ax,levels = [-1*t95, -1*t90, t90, t95], colors='none', \n",
    "                        hatches=['..', None, None, None, '..'], extend='both', add_colorbar=False, \n",
    "                        transform=ccrs.PlateCarree())\n",
    "    ax.set_extent([75, 290, -30, 30], crs=ccrs.PlateCarree())\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True)\n",
    "    gl.xlabels_top = False\n",
    "    gl.ylabels_left = True\n",
    "    gl.xlines = False\n",
    "    gl.ylines = False\n",
    "    gl.xlocator = mticker.FixedLocator([-90, -135, 180, 135, 90])\n",
    "    gl.ylocator = mticker.FixedLocator([-20, -10, 0, 10, 20])\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.xlabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    gl.ylabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.ylabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    ax.add_geometries([Ering], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth=2.5)\n",
    "    ax.add_geometries([Wring], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth =2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.title(var_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099d3d2-2607-4d74-917d-e097e43b3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "fig.dpi = 600\n",
    "for i in range(len(cors)):\n",
    "    tropcntl_plot(cors, cor_var_cntl)\n",
    "plt.suptitle('d18op' + ' Post Industrial PWC - PI CNTL Correlation', y =.95)\n",
    "plt.subplots_adjust(hspace=0, wspace=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(variable + \"glob_cor_post_anom.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92414b6c-b1b6-4750-a3b1-ea91b9652d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "fig.dpi = 600\n",
    "for i in range(len(cors)):\n",
    "    globcntl_plot(cors, cor_var_cntl)\n",
    "plt.suptitle('d18op' + ' Post Industrial PWC - PI CNTL Correlation', y =.95)\n",
    "plt.subplots_adjust(hspace=0, wspace=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(variable + \"glob_cor_post_anom.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94323f-4a14-49dd-8744-30ca851c3f78",
   "metadata": {},
   "source": [
    "# Calculate Linear Trend Per Grid Cell with significance stippling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa9b35-8503-4746-86b2-83e84255975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(r):\n",
    "    var_avg = r.mean(axis =0) \n",
    "    return(var_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e901f-fa0c-47bb-b21d-d3b7b7cbea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_trend(var,lon,lat,time,sig):\n",
    "    vt = []\n",
    "    vp = []\n",
    "    nlon=len(lon)\n",
    "    nlat=len(lat)\n",
    "    nt=len(time)\n",
    "    vart=zeros(nlat*nlon)\n",
    "    varp=zeros(nlat*nlon)\n",
    "    if len(var.shape)== 3:        \n",
    "        var1=reshape(var,(nt,nlat*nlon)) \n",
    "        for i in range(nlat*nlon):\n",
    "            v=var1[:,i]  \n",
    "            vart[i], intercept, r_value, varp[i], std_err=stats.linregress(time,v) # slope, intercept, r, p, std_error\n",
    "        vart=reshape(vart,(nlat,nlon))\n",
    "        varp=reshape(varp,(nlat,nlon))  \n",
    "        vt.append(vart)\n",
    "        vp.append(varp)\n",
    "        slope = np.stack(vt, axis=0)#, dim='ensemble')\n",
    "        pvalue = np.stack(vp, axis=0)#, dim='ensemble')\n",
    "        sig_pval = ma.masked_greater(pvalue, sig)\n",
    "        pval_avg = avg(sig_pval)\n",
    "        slope_avg = avg(slope)\n",
    "        return (slope, sig_pval, slope_avg, pval_avg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6359d-fb76-46d4-b10f-bd3f2daa08fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for ww in range(len(var_full)):\n",
    "    trend_full18, pval_full18, full_avg18, full_avgp18 = l_trend(anom_var18[ww,:,:,:].values,lon,lat,year,sig=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cebf0-b7a6-4462-86e7-c19b9c00d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vt = []\n",
    "vp = []\n",
    "for ww in range(len(var_ghg)):\n",
    "    trend_ghg, pval_ghg, ghg_avg, ghg_avgp = l_trend(anom_var_ghg18[ww,:,:,:].values,lon,lat,year,sig=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b1806-3826-4c73-85e8-ab6c2a10ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vt = []\n",
    "vp = []\n",
    "for ww in range(len(var_oza)):\n",
    "    trend_oza, pval_oza, oza_avg, oza_avgp = l_trend(anom_var_oza18[ww,:,:,:].values,lon,lat,year,sig=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a213297-3f8d-4711-a299-4ef8803b9ecd",
   "metadata": {},
   "source": [
    "# Plotting Linear Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdccda-547a-420b-8f0f-d7bfc1defded",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = [full_avg18, ghg_avg, oza_avg]\n",
    "lt_sig = [full_avgp18, ghg_avgp, oza_avgp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691480a3-1bc6-4987-87c8-078d1c5c75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trop_plot_trend(p,s):\n",
    "    ax = fig.add_subplot(2,2,i+1, projection=ccrs.Robinson(central_longitude=180))\n",
    "    ax.coastlines()\n",
    "    plt.contourf(lon,lat,p[i],cmap='bwr', add_colorbar=False, transform=ccrs.PlateCarree())\n",
    "    plt.pcolor(lon,lat,p[i], cmap='bwr', vmin=-1, vmax=1, transform=ccrs.PlateCarree())  \n",
    "    plt.contourf(lon,lat,s[i], colors='none',  hatches=['..', None, None, None, '..'], extend='both', add_colorbar=False, \n",
    "                        transform=ccrs.PlateCarree())\n",
    "    ax.set_extent([75, 290, -30, 30], crs=ccrs.PlateCarree())\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True)\n",
    "    gl.xlabels_top = False\n",
    "    gl.ylabels_left = True\n",
    "    gl.xlines = False\n",
    "    gl.ylines = False\n",
    "    gl.xlocator = mticker.FixedLocator([-90, -135, 180, 135, 90])\n",
    "    gl.ylocator = mticker.FixedLocator([-20, -10, 0, 10, 20])\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.xlabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    gl.ylabel_style = {'size': 25, 'color': 'gray'}\n",
    "    gl.ylabel_style = {'size': 12.5,'color': 'black', 'weight': 'bold'}\n",
    "    ax.add_geometries([Ering], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth=2.5)\n",
    "    ax.add_geometries([Wring], ccrs.PlateCarree(), facecolor='none', edgecolor='dimgrey', linewidth =2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.title(var_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0c4b8-dcfb-46eb-a3a4-46bc6e76dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_trend_plot(t, p):\n",
    "    ax = fig.add_subplot(2,3,i+1, projection=ccrs.Robinson(central_longitude=180))\n",
    "    ax.coastlines()\n",
    "    plt.contourf(lon,lat,t[i],cmap='bwr', transform=ccrs.PlateCarree()) \n",
    "    plt.pcolor(lon,lat,t[i], cmap='bwr', vmin=-.001, vmax=.001, transform=ccrs.PlateCarree())    \n",
    "    #plt.colorbar(orientation='horizontal')\n",
    "    cs = ax.contourf(lon,lat, p[i], colors='none' ,hatches=[\".\"],extend='lower', transform=ccrs.PlateCarree())\n",
    "    ax.set_aspect('equal')\n",
    "    plt.rcParams['font.size'] = '14'\n",
    "    plt.title(var_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a8c82-8756-455a-9335-6fd0158531b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "fig.dpi = 600\n",
    "for i in range(len(lt)):\n",
    "    glob_trend_plot(lt, lt_sig)\n",
    "plt.suptitle('dD Correlation with ' + region, y =.65)\n",
    "#plt.rcParams['font.size'] = '24'\n",
    "plt.subplots_adjust(hspace=-1, wspace=0)\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
    "plt.tight_layout()\n",
    "plt.savefig(variable + \"glob_cor_post_anom.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9acad-6fef-465f-8f26-b38ca6ea7745",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "fig.dpi = 600\n",
    "for i in range(len(lt)):\n",
    "    trop_plot_trend(lt, lt_sig)\n",
    "plt.suptitle('dD Linear Trend with ' + region, y =.65)\n",
    "#plt.rcParams['font.size'] = '24'\n",
    "plt.subplots_adjust(hspace=-1, wspace=0)\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
    "plt.tight_layout()\n",
    "plt.savefig(variable + \"glob_cor_post_anom.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c886251-34a0-4a63-b2b8-6ab0ff832c44",
   "metadata": {},
   "source": [
    "# Calculate Enemeble Average and Plot Time Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509539cb-7d14-46c4-996d-acf6cbdef579",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Calculate Enemeble Average for Time Series \n",
    "index_var = pwci(anom_index).mean(dim = 'ensemble')\n",
    "index_var_ghg = pwci(anom_index_ghg).mean(dim = 'ensemble')\n",
    "index_lulc = pwci(anom_index_lulc)\n",
    "index_var_oza = pwci(anom_index_oza).mean(dim = 'ensemble')\n",
    "index_var_cntl = pwci(anom_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691ee0c-7bc1-4875-b7c1-dcf12585d832",
   "metadata": {},
   "source": [
    "## Time Series Plotting Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3aa5b9-166a-438b-9e6f-40e201c8ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set list of time series to loop over\n",
    "ts_list = [index_var, index_var_ghg, index_var_oza]\n",
    "lent_post = len(ts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dba895-6ede-4cc4-82ef-451d2881cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'dD'\n",
    "var_post = ['FULL', 'GHG','OZA'] # OZA Removed\n",
    "dt = 1/12  # In years\n",
    "t_oza = np.arange(0,1872)*dt +1850\n",
    "colors = ['green', 'blue','darkorange']\n",
    "colors_line = ['green', 'blue','orange']\n",
    "colors_ribbon = ['lightgreen', 'lightblue','lightyellow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe5b12-4024-47a0-90ce-3527c122c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_post(r):\n",
    "   # ax = fig.add_subplot(sharex=True, sharey=False)\n",
    "    post = r[i].sel(time = slice('1850-01-01', '2005-12-01'))\n",
    "    res = stats.linregress(t_oza,post)\n",
    "    roll_avg = post.rolling(time=120, center=True).mean()\n",
    "    ax.set_title(var_post[i])\n",
    "    #ax.set_ylim([-.4,.4])\n",
    "    plt.rcParams['font.size'] = '30'\n",
    "    plt.plot(t_oza,roll_avg, color = colors[i], label = var_post[i])\n",
    "    plt.plot(t_oza, (res.intercept + res.slope*t_oza), colors_line[i])\n",
    "    plt.fill_between(t_oza, roll_avg + roll_avg.std(),roll_avg - roll_avg.std(),  edgecolor= colors_ribbon[i], facecolor=colors_ribbon[i], alpha=1)\n",
    "    plt.ylabel('dD (per mille)', fontsize = '28')\n",
    "    fig.suptitle('dD Difference East (210-270) - West (90-150) Anomalies')\n",
    "    #print('Slope Pre' + var_post[i] + res.slope)\n",
    "    #print('p-value' +var_post[i] + res.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a812765-471c-4ee0-85e7-95b15bd11f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Time\n",
    "fig = plt.figure(figsize=(40,20))\n",
    "for i in range(lent_post):\n",
    "    roll_post(ts_list)\n",
    "    plt.ylim(-5,5)#\n",
    "plt.xlabel('Year', fontsize = '30')\n",
    "plt.rcParams['font.size'] = '30'\n",
    "plt.rcParams['font.family']= 'sans-serif'\n",
    "plt.suptitle(variable +' 10 Year Rolling Average ' + region , y = 0.9, fontsize = '35',fontweight=\"bold\")\n",
    "#plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.legend(mode = \"expand\", loc = 'lower center', bbox_to_anchor=(0.25,-0.25,0.5, -0.75), ncol =5)\n",
    "plt.savefig(variable + \"roll_post_mc.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2024a",
   "language": "python",
   "name": "npl-2024a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
